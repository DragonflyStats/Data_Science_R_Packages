---
title: "``dlookr::normality.tbl_dbi``"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dlookr)
```

normality.tbl_dbi Performs the Shapiro-Wilk test of normality

### Description 

The normality() performs Shapiro-Wilk test of normality of numerical(INTEGER, NUMBER, etc.)
column of the DBMS table through tbl_dbi.

### Usage 
<pre><code>

## S3 method for class 'tbl_dbi'
normality(.data, ..., sample = 5000,
``in_database = FALSE``, collect_size = Inf)

</code></pre>

### Arguments
 
* `.data``:   a tbl_dbi.
... one or more unquoted expressions separated by commas. You can treat variable names like they are positions. Positive values select variables; negative values to drop variables. If the first expression is negative, normality() will automatically
start with all variables. These arguments are automatically quoted and evaluated in a context where column names represent column positions. They support
unquoting and splicing.
* ``sample``:  the numer of samples to perform the test.
* ``in_database``: Specifies whether to perform in-database operations. If TRUE, most operations are performed in the DBMS. if FALSE, table data is taken in R and operated in-memory. Not yet supported in_database = TRUE.
* ``collect_size``: a integer. The number of data samples from the DBMS to R. Applies only if ````in_database = FALSE````.
See vignette("EDA") for an introduction to these concepts.

### Details 

This function is useful when used with the group_by function of the dplyr package. If you want to
test by level of the categorical data you are interested in, rather than the whole observation, you can
use group_tf as the group_by function. This function is computed shapiro.test function.
normality.tbl_dbi 57

### Value 

An object of the same class as .data.
Normality test information
The information derived from the numerical data test is as follows.
*  statistic : the value of the Shapiro-Wilk statistic.
*  p_value : an approximate p-value for the test. This is said in Roystion(1995) to be adequate for ``p_value < 0.1``.
*  sample : the numer of samples to perform the test. The number of observations supported by
the stats::shapiro.test function is 3 to 5000.

### See Also
 
normality.data.frame, diagnose_numeric.tbl_dbi, describe.tbl_dbi, plot_normality.tbl_dbi.

### Examples
```{r}

library(dplyr)
# Generate data for the example
carseats <- ISLR::Carseats
carseats[sample(seq(NROW(carseats)), 20), "Income"] <- NA
carseats[sample(seq(NROW(carseats)), 5), "Urban"] <- NA
# connect DBMS
con_sqlite <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
# copy carseats to the DBMS with a table named TB_CARSEATS
copy_to(con_sqlite, carseats, name = "TB_CARSEATS", overwrite = TRUE)
```

```{r}
# Using pipes ---------------------------------
# Normality test of all numerical variables
con_sqlite %>%
tbl("TB_CARSEATS") %>%
normality()
# Positive values select variables, and In-memory mode and collect size is 200
con_sqlite %>%
tbl("TB_CARSEATS") %>%
normality(Sales, Price, collect_size = 200)
# Positions values select variables
con_sqlite %>%
tbl("TB_CARSEATS") %>%
normality(1)
```

```{r}
# Using pipes & dplyr -------------------------
# Test all numerical variables by 'ShelveLoc' and 'US',

# and extract only those with 'ShelveLoc' variable level is "Good".
con_sqlite %>%
tbl("TB_CARSEATS") %>%
group_by(ShelveLoc, US) %>%
normality() %>%
filter(ShelveLoc == "Good")
# extract only those with 'Urban' variable level is "Yes",
# and test 'Sales' by 'ShelveLoc' and 'US'
con_sqlite %>%
tbl("TB_CARSEATS") %>%
filter(Urban == "Yes") %>%
group_by(ShelveLoc, US) %>%
normality(Sales)
```

```{r}
# Test log(Income) variables by 'ShelveLoc' and 'US',
# and extract only p.value greater than 0.01.
# SQLite extension functions for log
RSQLite::initExtension(con_sqlite)
con_sqlite %>%
tbl("TB_CARSEATS") %>%
mutate(log_income = log(Income)) %>%
group_by(ShelveLoc, US) %>%
normality(log_income) %>%
filter(p_value > 0.01)
```